{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M+ Model - HotpotQA Multi-hop QA Evaluation\n",
    "\n",
    "논문 코드(longbench_pred.py) 기반으로 M+ 모델 지원 추가  \n",
    "L4 GPU (24GB) 메모리에 맞게 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files removed: 1039 (6389.8 MB)\n",
      "Found existing installation: pyarrow 14.0.0\n",
      "Can't uninstall 'pyarrow'. No files were found to uninstall.\n",
      "Found existing installation: pandas 2.0.3\n",
      "Uninstalling pandas-2.0.3:\n",
      "  Successfully uninstalled pandas-2.0.3\n",
      "Found existing installation: datasets 2.18.0\n",
      "Uninstalling datasets-2.18.0:\n",
      "  Successfully uninstalled datasets-2.18.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/lhe339/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pyarrow==14.0.0 in /home/lhe339/.venv/lib/python3.9/site-packages (14.0.0)\n",
      "Collecting pandas==2.0.3\n",
      "  Downloading pandas-2.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting datasets==2.18.0\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/lhe339/.venv/lib/python3.9/site-packages (from pandas==2.0.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/lhe339/.venv/lib/python3.9/site-packages (from pandas==2.0.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/lhe339/.venv/lib/python3.9/site-packages (from pandas==2.0.3) (2025.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/lhe339/.venv/lib/python3.9/site-packages (from pandas==2.0.3) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (3.19.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (0.7)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets==2.18.0) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (0.36.0)\n",
      "Requirement already satisfied: packaging in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lhe339/.venv/lib/python3.9/site-packages (from datasets==2.18.0) (6.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from aiohttp->datasets==2.18.0) (1.22.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from multidict<7.0,>=4.5->aiohttp->datasets==2.18.0) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.0 in /home/lhe339/.venv/lib/python3.9/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.18.0) (3.11)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/lhe339/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/lhe339/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/lhe339/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.18.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lhe339/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.18.0) (2.6.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lhe339/.venv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.18.0) (2026.1.4)\n",
      "Downloading pandas-2.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/lhe339/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pandas, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [datasets]1/2\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2K\u001b[33mWARNING: Ignoring invalid distribution -yarrow (/home/lhe339/.venv/lib/python3.9/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed datasets-2.18.0 pandas-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip cache purge\n",
    "!pip uninstall pyarrow pandas datasets -y\n",
    "!pip install pyarrow==14.0.0 pandas==2.0.3 datasets==2.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lhe339/.venv/lib/python3.9/site-packages/torch/cuda/__init__.py:54: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyarrow' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgc\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/datasets/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.18.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py:59\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpc\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/pandas/compat/__init__.py:29\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     26\u001b[0m     is_numpy_dev,\n\u001b[1;32m     27\u001b[0m     np_version_under1p21,\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     30\u001b[0m     pa_version_under7p0,\n\u001b[1;32m     31\u001b[0m     pa_version_under8p0,\n\u001b[1;32m     32\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     33\u001b[0m     pa_version_under11p0,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_function_name\u001b[39m(f: F, name: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m F:\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m    Bind the name/qualname attributes of the function.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/lib/python3.9/site-packages/pandas/compat/pyarrow.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     _pa_version \u001b[38;5;241m=\u001b[39m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__version__\u001b[49m\n\u001b[1;32m     11\u001b[0m     _palv \u001b[38;5;241m=\u001b[39m Version(_pa_version)\n\u001b[1;32m     12\u001b[0m     pa_version_under7p0 \u001b[38;5;241m=\u001b[39m _palv \u001b[38;5;241m<\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import gc\n",
    "\n",
    "# 현재 디렉토리를 path에 추가 (modeling_mplus import 위해)\n",
    "sys.path.insert(0, os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "CONFIG = {\n",
    "    'model_path': 'YuWangX/mplus-8b',\n",
    "    'num_samples': 100,\n",
    "    'max_gen': 32,\n",
    "    'split_model': True,\n",
    "    'seed': 42,\n",
    "    'output_dir': './results_hotpotqa',\n",
    "    'chunk_size': 256,  # 메모리 절약을 위해 256으로 설정\n",
    "    'dataset': 'hotpotqa'\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"재현성을 위한 시드 설정\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"정답 정규화 (SQuAD 스타일)\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    \"\"\"F1 스코어 계산\"\"\"\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "\n",
    "    if len(prediction_tokens) == 0 or len(ground_truth_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    \"\"\"Exact Match 스코어 계산\"\"\"\n",
    "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, ground_truths):\n",
    "    \"\"\"전체 메트릭 계산\"\"\"\n",
    "    em_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for pred, truths in zip(predictions, ground_truths):\n",
    "        if isinstance(truths, list):\n",
    "            em = max(exact_match_score(pred, truth) for truth in truths)\n",
    "            f1 = max(f1_score(pred, truth) for truth in truths)\n",
    "        else:\n",
    "            em = exact_match_score(pred, truths)\n",
    "            f1 = f1_score(pred, truths)\n",
    "\n",
    "        em_scores.append(em)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    return {\n",
    "        'exact_match': np.mean(em_scores) * 100,\n",
    "        'f1': np.mean(f1_scores) * 100,\n",
    "        'total_samples': len(predictions)\n",
    "    }\n",
    "\n",
    "\n",
    "def get_prompt_format(dataset_name):\n",
    "    \"\"\"데이터셋별 프롬프트 포맷\"\"\"\n",
    "    prompts = {\n",
    "        \"hotpotqa\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "        \"2wikimqa\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "        \"musique\": \"Answer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nThe following are given passages.\\n{context}\\n\\nAnswer the question based on the given passages. Only give me the answer and do not output any other words.\\n\\nQuestion: {input}\\nAnswer:\",\n",
    "    }\n",
    "    return prompts.get(dataset_name, prompts[\"hotpotqa\"])\n",
    "\n",
    "print(\"Helper functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seed_everything' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseed_everything\u001b[49m(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeed set to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seed_everything' is not defined"
     ]
    }
   ],
   "source": [
    "seed_everything(CONFIG['seed'])\n",
    "print(f\"Seed set to {CONFIG['seed']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {CONFIG['dataset']} dataset from LongBench...\")\n",
    "\n",
    "full_dataset = load_dataset(\n",
    "    'THUDM/LongBench',\n",
    "    CONFIG['dataset'],\n",
    "    split='test',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 샘플 수 제한\n",
    "if CONFIG['num_samples'] and CONFIG['num_samples'] < len(full_dataset):\n",
    "    indices = list(range(CONFIG['num_samples']))\n",
    "    dataset = full_dataset.select(indices)\n",
    "else:\n",
    "    dataset = full_dataset\n",
    "\n",
    "print(f\"Loaded {len(dataset)} samples\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(f\"  Question: {dataset[0]['input'][:100]}...\")\n",
    "print(f\"  Answers: {dataset[0]['answers']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_mplus import MPlus\n",
    "\n",
    "print(f\"Loading M+ model from {CONFIG['model_path']}...\")\n",
    "print(f\"Split model: {CONFIG['split_model']}\")\n",
    "\n",
    "# GPU 메모리 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if CONFIG['split_model']:\n",
    "    model = MPlus.from_pretrained(\n",
    "        CONFIG['model_path'],\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    model = MPlus.from_pretrained(\n",
    "        CONFIG['model_path'],\n",
    "        torch_dtype=torch.bfloat16\n",
    "    ).cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# 메모리 사용량 출력\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"GPU Memory: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/llama-3-8b\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = get_prompt_format(CONFIG['dataset'])\n",
    "\n",
    "# 모델의 device 확인\n",
    "if hasattr(model, 'device'):\n",
    "    device = model.device\n",
    "else:\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "# 초기 메모리 백업 (논문 방식)\n",
    "backup_memory = model.memory.clone().detach().cpu()\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "results_detail = []\n",
    "\n",
    "print(f\"\\nEvaluating {len(dataset)} samples...\")\n",
    "print(f\"Chunk size: {CONFIG['chunk_size']}\")\n",
    "\n",
    "for idx, sample in enumerate(tqdm(dataset, desc=\"Evaluating\")):\n",
    "    try:\n",
    "        # 메모리 복원 (각 샘플마다 초기 상태로)\n",
    "        model.memory.data = backup_memory.clone().detach().to(model.memory.device)\n",
    "\n",
    "        # 프롬프트 생성\n",
    "        prompt = prompt_format.format(context=sample['context'], input=sample['input'])\n",
    "\n",
    "        # 토큰화\n",
    "        prompt_ids = tokenizer(prompt, add_special_tokens=False, truncation=False).input_ids\n",
    "\n",
    "        # 컨텍스트를 청크로 분할하여 메모리에 주입\n",
    "        contexts_ids = []\n",
    "        remaining_ids = prompt_ids.copy()\n",
    "\n",
    "        # 마지막 청크는 생성을 위해 남겨둠\n",
    "        while len(remaining_ids) > CONFIG['chunk_size']:\n",
    "            contexts_ids.append(remaining_ids[:CONFIG['chunk_size']])\n",
    "            remaining_ids = remaining_ids[CONFIG['chunk_size']:]\n",
    "\n",
    "        # 마지막 부분은 sentence로 사용 (생성 입력)\n",
    "        sentence_ids = remaining_ids if remaining_ids else contexts_ids.pop()\n",
    "\n",
    "        # 메모리 주입\n",
    "        with torch.no_grad():\n",
    "            for context_chunk in contexts_ids:\n",
    "                context_tensor = torch.tensor(context_chunk).unsqueeze(0).cuda()\n",
    "                attention_mask = torch.ones(len(context_chunk) + model.num_tokens).long().unsqueeze(0).cuda()\n",
    "\n",
    "                model.inject_memory(\n",
    "                    context_tensor,\n",
    "                    attention_mask,\n",
    "                    update_memory=True\n",
    "                )\n",
    "\n",
    "            # 생성\n",
    "            sentence_tensor = torch.tensor(sentence_ids).unsqueeze(0).cuda()\n",
    "            gen_attention_mask = torch.ones(\n",
    "                len(sentence_ids) + model.num_blocks * model.num_tokens\n",
    "            ).unsqueeze(0).long().cuda()\n",
    "\n",
    "            output = model.generate(\n",
    "                input_ids=sentence_tensor,\n",
    "                attention_mask=gen_attention_mask,\n",
    "                max_new_tokens=CONFIG['max_gen'],\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                temperature=1.0,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )[0]\n",
    "\n",
    "            # 생성된 부분만 추출\n",
    "            pred = tokenizer.decode(output[len(sentence_ids):], skip_special_tokens=True)\n",
    "\n",
    "        # 정답 추출\n",
    "        answers = sample['answers']\n",
    "        if isinstance(answers, str):\n",
    "            answers = [answers]\n",
    "\n",
    "        predictions.append(pred.strip())\n",
    "        ground_truths.append(answers)\n",
    "\n",
    "        # 상세 결과 저장\n",
    "        results_detail.append({\n",
    "            'idx': idx,\n",
    "            'question': sample['input'],\n",
    "            'prediction': pred.strip(),\n",
    "            'ground_truth': answers,\n",
    "            'context_length': len(prompt_ids)\n",
    "        })\n",
    "\n",
    "        # 매 샘플마다 메모리 정리\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError at sample {idx}: {e}\")\n",
    "        predictions.append(\"\")\n",
    "        ground_truths.append(sample.get('answers', [\"\"]))\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(predictions, ground_truths)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset: {CONFIG['dataset']}\")\n",
    "print(f\"Total Samples: {metrics['total_samples']}\")\n",
    "print(f\"Exact Match: {metrics['exact_match']:.2f}%\")\n",
    "print(f\"F1 Score: {metrics['f1']:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "output_file = os.path.join(\n",
    "    CONFIG['output_dir'],\n",
    "    f\"mplus_{CONFIG['dataset']}_n{CONFIG['num_samples']}_seed{CONFIG['seed']}.json\"\n",
    ")\n",
    "\n",
    "results = {\n",
    "    'config': CONFIG,\n",
    "    'metrics': metrics,\n",
    "    'predictions': results_detail\n",
    "}\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAMPLE PREDICTIONS (first 10)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, detail in enumerate(results_detail[:10]):\n",
    "    print(f\"\\n[{i+1}] Question: {detail['question'][:100]}...\")\n",
    "    print(f\"    Prediction: {detail['prediction']}\")\n",
    "    print(f\"    Ground Truth: {detail['ground_truth']}\")\n",
    "    \n",
    "    # EM/F1 계산\n",
    "    em = max(exact_match_score(detail['prediction'], gt) for gt in detail['ground_truth'])\n",
    "    f1 = max(f1_score(detail['prediction'], gt) for gt in detail['ground_truth'])\n",
    "    print(f\"    EM: {em}, F1: {f1:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.9.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
