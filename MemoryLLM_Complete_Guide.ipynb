{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MemoryLLM: Self-Updatable Large Language Models 완전 가이드\n",
    "\n",
    "이 노트북은 **MemoryLLM**의 Training과 Inference 코드를 상세히 분석하고 설명합니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 논문 정보\n",
    "\n",
    "| 항목 | 내용 |\n",
    "|------|------|\n",
    "| **제목** | MemoryLLM: Towards Self-Updatable Large Language Models |\n",
    "| **학회** | ICML 2024 |\n",
    "| **저자** | Yu Wang et al. |\n",
    "| **GitHub** | [YuWangX/MemoryLLM](https://github.com/YuWangX/MemoryLLM) |\n",
    "| **HuggingFace** | [YuWangX/memoryllm-7b](https://huggingface.co/YuWangX/memoryllm-7b) |\n",
    "\n",
    "---\n",
    "\n",
    "## 핵심 기여\n",
    "\n",
    "MemoryLLM은 기존 LLM의 한계를 극복하기 위해 **Self-Updatable Memory Pool**을 도입했습니다:\n",
    "\n",
    "1. **지식 업데이트 문제**: 기존 LLM은 학습 이후 새로운 지식을 반영하기 어려움\n",
    "2. **긴 컨텍스트 처리**: 제한된 context window로 인한 정보 손실\n",
    "3. **메모리 효율성**: 중요한 정보를 압축하여 저장하고 필요할 때 검색\n",
    "\n",
    "### 핵심 아이디어\n",
    "\n",
    "```\n",
    "기존 LLM:     입력 → [Transformer Layers] → 출력\n",
    "MemoryLLM:    입력 → [Memory Pool + Transformer Layers] → 출력\n",
    "                         ↑\n",
    "                    Self-Update\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 아키텍처 개요\n",
    "\n",
    "MemoryLLM의 핵심 구조는 다음과 같습니다:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      MemoryLLM                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│   ┌─────────────────────────────────────────────────────┐   │\n",
    "│   │              Memory Pool (Self-Updatable)           │   │\n",
    "│   │   Shape: [L, num_blocks × num_tokens, hidden_size]  │   │\n",
    "│   │   예시: [32, 50 × 256, 4096] = 1.67B parameters     │   │\n",
    "│   └─────────────────────────────────────────────────────┘   │\n",
    "│                         ↓ (concatenate)                     │\n",
    "│   ┌─────────────────────────────────────────────────────┐   │\n",
    "│   │            Transformer Decoder Layers               │   │\n",
    "│   │      (LLaMA-2/3 기반, 32 layers, 4096 dim)          │   │\n",
    "│   └─────────────────────────────────────────────────────┘   │\n",
    "│                         ↓                                   │\n",
    "│   ┌─────────────────────────────────────────────────────┐   │\n",
    "│   │                    LM Head                          │   │\n",
    "│   │              (Vocabulary Projection)                │   │\n",
    "│   └─────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### 핵심 컴포넌트\n",
    "\n",
    "| 컴포넌트 | 역할 | 크기 |\n",
    "|---------|------|------|\n",
    "| **Memory Pool** | 컨텍스트 정보 저장 | [32, 12800, 4096] |\n",
    "| **BOS Embedding** | 각 레이어의 시작 토큰 | [32, 1, 4096] |\n",
    "| **Positional Embedding** | 새 메모리 위치 인코딩 | [1, 1, 4096] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 메모리 업데이트 메커니즘\n",
    "\n",
    "MemoryLLM의 핵심은 **inject_memory → update_memory** 사이클입니다:\n",
    "\n",
    "```\n",
    "┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "│   Context    │ --> │ inject_memory│ --> │ delta_memory │\n",
    "│  (새 정보)    │     │    (주입)    │     │  (추출된 표현) │\n",
    "└──────────────┘     └──────────────┘     └──────────────┘\n",
    "                                                  │\n",
    "                                                  ↓\n",
    "┌──────────────┐     ┌──────────────┐     ┌──────────────┐\n",
    "│ Memory Pool  │ <-- │update_memory │ <-- │ drop_memory  │\n",
    "│   (갱신됨)   │     │   (저장)     │     │  (공간 확보)  │\n",
    "└──────────────┘     └──────────────┘     └──────────────┘\n",
    "```\n",
    "\n",
    "### 단계별 설명\n",
    "\n",
    "1. **inject_memory**: 새로운 context를 모델에 통과시켜 각 레이어의 hidden states 추출\n",
    "2. **delta_memory**: 추출된 표현 (shape: `[batch, L, num_tokens, d]`)\n",
    "3. **drop_memory**: 기존 메모리에서 1/num_blocks 만큼 랜덤 제거\n",
    "4. **update_memory**: 새 delta_memory를 메모리 풀 끝에 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 환경 설정\n",
    "\n",
    "이 노트북을 실행하기 위해 필요한 환경을 설정합니다.\n",
    "\n",
    "### 하드웨어 요구사항\n",
    "\n",
    "| 모델 | GPU VRAM | 권장 GPU |\n",
    "|------|----------|----------|\n",
    "| memoryllm-7b | ~16GB | RTX 3090, A100 |\n",
    "| memoryllm-8b | ~18GB | RTX 4090, A100 |\n",
    "| Training | ~40GB+ | A100 80GB |\n",
    "\n",
    "### 소프트웨어 버전\n",
    "\n",
    "| 패키지 | 버전 |\n",
    "|--------|------|\n",
    "| Python | 3.10+ |\n",
    "| PyTorch | 2.2-2.5 |\n",
    "| Transformers | 4.40-4.48 |\n",
    "| Flash Attention | 2.x |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: 의존성 설치\n",
    "# 이미 설치되어 있다면 이 셀을 건너뛰세요\n",
    "\n",
    "# 기본 의존성\n",
    "# !pip install torch>=2.2.0 transformers>=4.40.0\n",
    "\n",
    "# 추가 의존성\n",
    "# !pip install peft pytorch-lightning omegaconf einops\n",
    "\n",
    "# Flash Attention 2 (선택사항, 성능 향상)\n",
    "# !pip install flash-attn --no-build-isolation\n",
    "\n",
    "print(\"의존성 설치 완료! (이미 설치된 경우 건너뛰기)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: 기본 import 및 환경 확인\n",
    "\"\"\"\n",
    "이 셀에서는 필요한 라이브러리를 import하고 환경을 확인합니다.\n",
    "\n",
    "주요 라이브러리:\n",
    "- torch: 딥러닝 프레임워크\n",
    "- transformers: HuggingFace의 LLM 라이브러리\n",
    "- matplotlib: 시각화\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "# 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "# 환경 확인\n",
    "print(f\"Python 버전: {sys.version}\")\n",
    "print(f\"PyTorch 버전: {torch.__version__}\")\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA 버전: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU 메모리: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: 프로젝트 경로 설정 및 MemoryLLM import\n",
    "\"\"\"\n",
    "MemoryLLM 저장소의 코드를 import하기 위해 경로를 설정합니다.\n",
    "\n",
    "파일 구조:\n",
    "MemoryLLM/\n",
    "├── modeling_memoryllm.py      # 추론용 모델 (이 파일을 import)\n",
    "├── modeling_mplus.py          # M+ 확장 모델\n",
    "├── configuration_memoryllm.py # 설정 클래스\n",
    "├── test_qa_memory.py          # 평가 스크립트\n",
    "└── train/                     # 학습 코드\n",
    "\"\"\"\n",
    "\n",
    "# 프로젝트 루트 경로 설정\n",
    "PROJECT_ROOT = os.path.dirname(os.path.abspath('__file__'))  # 현재 노트북 위치\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"프로젝트 루트: {PROJECT_ROOT}\")\n",
    "\n",
    "# MemoryLLM 관련 모듈 import\n",
    "try:\n",
    "    from modeling_memoryllm import MemoryLLM\n",
    "    from configuration_memoryllm import MemoryLLMConfig\n",
    "    print(\"✓ MemoryLLM 모듈 import 성공!\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import 실패: {e}\")\n",
    "    print(\"  → 이 노트북이 MemoryLLM 저장소 루트에 있는지 확인하세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Tokenizer 로드\n",
    "\"\"\"\n",
    "LLaMA 기반 토크나이저를 로드합니다.\n",
    "\n",
    "MemoryLLM은 LLaMA-2/3 기반이므로 해당 토크나이저를 사용합니다.\n",
    "- LlamaTokenizer: 기본 LLaMA 토크나이저\n",
    "- AutoTokenizer: HuggingFace의 자동 토크나이저 (권장)\n",
    "\n",
    "주의: 실제 모델을 로드하려면 HuggingFace 로그인이 필요할 수 있습니다.\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 모델 경로 (HuggingFace Hub 또는 로컬 경로)\n",
    "MODEL_PATH = \"YuWangX/memoryllm-8b\"  # 또는 로컬 경로\n",
    "\n",
    "# 토크나이저만 먼저 로드 (모델보다 가벼움)\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    \n",
    "    # 특수 토큰 확인\n",
    "    print(f\"Vocabulary 크기: {tokenizer.vocab_size}\")\n",
    "    print(f\"PAD 토큰: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})\")\n",
    "    print(f\"BOS 토큰: {tokenizer.bos_token} (ID: {tokenizer.bos_token_id})\")\n",
    "    print(f\"EOS 토큰: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})\")\n",
    "    print(\"✓ 토크나이저 로드 성공!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ 토크나이저 로드 실패: {e}\")\n",
    "    print(\"  → HuggingFace에 로그인하거나 로컬 모델 경로를 사용하세요.\")\n",
    "    tokenizer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: 토큰화 예시\n",
    "\"\"\"\n",
    "토크나이저 사용 예시입니다.\n",
    "\n",
    "MemoryLLM에서 토큰화는 두 가지 목적으로 사용됩니다:\n",
    "1. Context 주입: inject_memory()에 전달할 context 토큰화\n",
    "2. 질문/응답: 생성을 위한 입력 토큰화\n",
    "\"\"\"\n",
    "\n",
    "if tokenizer is not None:\n",
    "    # 예시 텍스트\n",
    "    context = \"Last week, John had a wonderful picnic with David. During the picnic, David mentioned that his favorite fruit is strawberry.\"\n",
    "    question = \"What fruit does David like?\"\n",
    "    \n",
    "    # 토큰화\n",
    "    context_tokens = tokenizer(context, return_tensors='pt')\n",
    "    question_tokens = tokenizer(question, return_tensors='pt')\n",
    "    \n",
    "    print(\"=== Context 토큰화 결과 ===\")\n",
    "    print(f\"원문: {context}\")\n",
    "    print(f\"토큰 수: {context_tokens['input_ids'].shape[1]}\")\n",
    "    print(f\"토큰 ID (처음 10개): {context_tokens['input_ids'][0][:10].tolist()}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"=== Question 토큰화 결과 ===\")\n",
    "    print(f\"원문: {question}\")\n",
    "    print(f\"토큰 수: {question_tokens['input_ids'].shape[1]}\")\n",
    "    print(f\"토큰 ID: {question_tokens['input_ids'][0].tolist()}\")\n",
    "else:\n",
    "    print(\"토크나이저가 로드되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Phase 1 완료!\n",
    "\n",
    "이 섹션에서 다룬 내용:\n",
    "- MemoryLLM 논문 소개 및 핵심 기여\n",
    "- 아키텍처 개요\n",
    "- 메모리 업데이트 메커니즘\n",
    "- 환경 설정 및 기본 import\n",
    "- 토크나이저 로드 및 사용 예시\n",
    "\n",
    "다음 섹션에서는 **Core Architecture**를 상세히 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 2: Core Architecture Deep Dive\n\n이 섹션에서는 MemoryLLM의 핵심 아키텍처를 상세히 분석합니다.\n\n## 2.1 Memory Pool 구조 상세 분석\n\nMemory Pool은 MemoryLLM의 핵심 컴포넌트입니다. 각 차원의 의미를 정확히 이해하는 것이 중요합니다.\n\n### Memory Pool Shape: `[L, num_blocks × num_tokens, hidden_size]`\n\n```\nMemory Pool 구조 시각화:\n\nLayer 0:  [████████████████████████████████████████] ← num_blocks × num_tokens = 12800 tokens\nLayer 1:  [████████████████████████████████████████]\nLayer 2:  [████████████████████████████████████████]\n  ...\nLayer 31: [████████████████████████████████████████]\n\n각 레이어: [num_blocks × num_tokens, hidden_size]\n         = [50 × 256, 4096]\n         = [12800, 4096]\n```\n\n### 차원별 의미\n\n| 차원 | 변수명 | 기본값 | 의미 |\n|------|--------|--------|------|\n| **L** | `num_hidden_layers` | 32 | Transformer 레이어 수. 각 레이어마다 독립적인 메모리 슬라이스 보유 |\n| **num_blocks** | `num_blocks` | 50 | 메모리 \"창문\" 개수. Drop 시 1/num_blocks 만큼 제거 |\n| **num_tokens** | `num_tokens` | 256 | 한 번의 inject로 추가되는 토큰 수 |\n| **hidden_size** | `hidden_size` | 4096 | 각 토큰의 표현 차원 (LLaMA-7B 기준) |\n\n### 메모리 크기 계산\n\n```\n총 파라미터 = L × num_blocks × num_tokens × hidden_size\n           = 32 × 50 × 256 × 4096\n           = 1,677,721,600 ≈ 1.68B parameters\n           ≈ 6.7 GB (float32) / 3.35 GB (float16)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 10: Memory Pool 구조 시각화\n\"\"\"\nMemory Pool의 구조를 시각적으로 이해하기 위한 함수입니다.\n\n이 시각화는 다음을 보여줍니다:\n- 각 레이어별 메모리 슬라이스\n- num_blocks 단위의 메모리 블록\n- inject 시 새로 추가되는 영역\n\"\"\"\n\ndef visualize_memory_structure(num_layers=32, num_blocks=50, num_tokens=256, \n                                highlight_new=True, show_layers=8):\n    \"\"\"\n    Memory Pool 구조를 시각화합니다.\n    \n    Args:\n        num_layers: Transformer 레이어 수\n        num_blocks: 메모리 블록 수\n        num_tokens: 블록당 토큰 수\n        highlight_new: 새로 추가될 영역 강조\n        show_layers: 표시할 레이어 수 (전체 표시하면 너무 김)\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(14, 6))\n    \n    total_tokens = num_blocks * num_tokens\n    \n    # 각 레이어별 메모리 바 그리기\n    for i in range(show_layers):\n        y = show_layers - 1 - i\n        \n        # 기존 메모리 (회색)\n        ax.barh(y, total_tokens - num_tokens, left=0, height=0.8, \n                color='lightblue', edgecolor='navy', alpha=0.7)\n        \n        # 새로 추가될 메모리 (강조)\n        if highlight_new:\n            ax.barh(y, num_tokens, left=total_tokens - num_tokens, height=0.8,\n                    color='coral', edgecolor='darkred', alpha=0.8)\n        \n        # 레이어 라벨\n        ax.text(-500, y, f'Layer {i}', ha='right', va='center', fontsize=10)\n    \n    # ... 표시 (생략된 레이어)\n    if num_layers > show_layers:\n        ax.text(total_tokens / 2, -0.8, f'... (Layer {show_layers} ~ {num_layers-1})', \n                ha='center', fontsize=10, style='italic')\n    \n    # 블록 경계선 표시 (처음 5개 블록만)\n    for b in range(6):\n        x = b * num_tokens\n        ax.axvline(x=x, color='gray', linestyle='--', alpha=0.3)\n        if b < 5:\n            ax.text(x + num_tokens/2, show_layers + 0.3, f'Block {b}', \n                    ha='center', fontsize=8, color='gray')\n    \n    # 범례\n    existing_patch = mpatches.Patch(color='lightblue', label=f'기존 메모리 ({num_blocks-1} blocks)')\n    new_patch = mpatches.Patch(color='coral', label=f'새 메모리 (1 block = {num_tokens} tokens)')\n    ax.legend(handles=[existing_patch, new_patch], loc='upper right')\n    \n    # 축 설정\n    ax.set_xlim(-1500, total_tokens + 500)\n    ax.set_ylim(-1.5, show_layers + 1)\n    ax.set_xlabel('Token Position')\n    ax.set_title(f'Memory Pool Structure: [{num_layers}, {num_blocks}×{num_tokens}, hidden_size]')\n    ax.set_yticks([])\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 메모리 크기 정보 출력\n    print(f\"\\n=== Memory Pool 정보 ===\")\n    print(f\"Shape: [{num_layers}, {total_tokens}, 4096]\")\n    print(f\"총 토큰 수: {num_layers * total_tokens:,}\")\n    print(f\"파라미터 수: {num_layers * total_tokens * 4096:,} ({num_layers * total_tokens * 4096 / 1e9:.2f}B)\")\n\n# 시각화 실행\nvisualize_memory_structure()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.2 MemoryLLMConfig 상세 분석\n\n`MemoryLLMConfig`는 LLaMA의 `PretrainedConfig`를 상속받아 MemoryLLM 특화 파라미터를 추가합니다.\n\n### 설정 파일 구조 (`configuration_memoryllm.py`)\n\n```python\nclass MemoryLLMConfig(PretrainedConfig):\n    model_type = \"memoryllm\"\n    \n    def __init__(\n        self,\n        # === LLaMA 기본 파라미터 ===\n        vocab_size=32000,           # 어휘 크기\n        hidden_size=4096,           # 은닉층 차원\n        intermediate_size=11008,    # MLP 중간층 차원\n        num_hidden_layers=32,       # Transformer 레이어 수\n        num_attention_heads=32,     # 어텐션 헤드 수\n        num_key_value_heads=None,   # GQA용 KV 헤드 수\n        \n        # === MemoryLLM 특화 파라미터 ===\n        num_tokens=256,             # 한 번에 주입되는 토큰 수\n        num_memory_tokens=12800,    # 총 메모리 토큰 수 (num_blocks × num_tokens)\n        num_blocks=50,              # 메모리 블록 수\n        add_bos_embedding=True,     # BOS 임베딩 추가 여부\n        drop_memory_per_layer=False, # 레이어별 독립적 메모리 드롭\n        add_decoder_lora=False,     # 디코더 LoRA 추가\n        lora_config=None,           # LoRA 설정\n        ...\n    )\n```\n\n### MemoryLLM 특화 파라미터 설명\n\n| 파라미터 | 기본값 | 설명 |\n|---------|--------|------|\n| `num_tokens` | 256 | `inject_memory()` 호출 시 추출되는 delta_memory의 토큰 수 |\n| `num_memory_tokens` | 12800 | 전체 메모리 풀의 토큰 수 (`num_blocks × num_tokens`) |\n| `num_blocks` | 50 | 메모리 슬라이딩 윈도우 크기. Drop 시 1/50 = 2% 제거 |\n| `add_bos_embedding` | True | 각 레이어 메모리 앞에 BOS 임베딩 추가 |\n| `drop_memory_per_layer` | False | True면 레이어마다 다른 인덱스 드롭 |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 12: MemoryLLMConfig 생성 및 확인\n\"\"\"\nMemoryLLMConfig를 직접 생성하여 파라미터를 확인합니다.\n\n이 설정은 모델 초기화 시 다음을 결정합니다:\n- Memory Pool의 크기와 구조\n- 어텐션 메커니즘 설정\n- 메모리 드롭 전략\n\"\"\"\n\n# 기본 설정으로 Config 생성\nconfig = MemoryLLMConfig(\n    # LLaMA 기본 설정\n    vocab_size=32000,\n    hidden_size=4096,\n    intermediate_size=11008,\n    num_hidden_layers=32,\n    num_attention_heads=32,\n    \n    # MemoryLLM 특화 설정\n    num_tokens=256,         # 한 번에 주입되는 토큰 수\n    num_blocks=50,          # 메모리 블록 수\n    num_memory_tokens=256 * 50,  # 자동 계산: 12800\n    add_bos_embedding=True,\n    drop_memory_per_layer=False,\n)\n\nprint(\"=== MemoryLLMConfig 설정 확인 ===\\n\")\n\n# LLaMA 기본 설정\nprint(\"[LLaMA 기본 설정]\")\nprint(f\"  vocab_size: {config.vocab_size}\")\nprint(f\"  hidden_size: {config.hidden_size}\")\nprint(f\"  intermediate_size: {config.intermediate_size}\")\nprint(f\"  num_hidden_layers: {config.num_hidden_layers}\")\nprint(f\"  num_attention_heads: {config.num_attention_heads}\")\nprint()\n\n# MemoryLLM 특화 설정\nprint(\"[MemoryLLM 특화 설정]\")\nprint(f\"  num_tokens: {config.num_tokens}\")\nprint(f\"  num_blocks: {config.num_blocks}\")\nprint(f\"  num_memory_tokens: {config.num_memory_tokens}\")\nprint(f\"  add_bos_embedding: {config.add_bos_embedding}\")\nprint(f\"  drop_memory_per_layer: {config.drop_memory_per_layer}\")\nprint()\n\n# 계산된 값\nprint(\"[계산된 메모리 정보]\")\nmemory_params = config.num_hidden_layers * config.num_memory_tokens * config.hidden_size\nprint(f\"  Memory Pool Shape: [{config.num_hidden_layers}, {config.num_memory_tokens}, {config.hidden_size}]\")\nprint(f\"  Memory Parameters: {memory_params:,} ({memory_params / 1e9:.2f}B)\")\nprint(f\"  Drop 비율: 1/{config.num_blocks} = {1/config.num_blocks*100:.1f}%\")\nprint(f\"  Drop 토큰 수: {config.num_memory_tokens // config.num_blocks} tokens\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.3 MemoryLLM 클래스 구조\n\n`MemoryLLM`은 HuggingFace의 `LlamaForCausalLM`을 상속받아 메모리 기능을 추가합니다.\n\n### 클래스 계층 구조\n\n```\nMemoryLLM (modeling_memoryllm.py)\n    └── LlamaForCausalLM (transformers)\n            └── LlamaPreTrainedModel\n                    └── PreTrainedModel\n```\n\n### `__init__` 메서드 분석 (Line 1512-1550)\n\n```python\nclass MemoryLLM(LlamaForCausalLM):\n    def __init__(self, config):\n        # 1. LLaMA 기본 초기화\n        LlamaForCausalLM.__init__(self, config)\n        \n        # 2. Config에서 핵심 파라미터 추출\n        self.L = config.num_hidden_layers      # 레이어 수 (32)\n        self.d = config.hidden_size            # 은닉층 차원 (4096)\n        self.num_blocks = config.num_blocks    # 메모리 블록 수 (50)\n        self.num_tokens = config.num_tokens    # 블록당 토큰 수 (256)\n        \n        # 3. Memory Pool 초기화 (핵심!)\n        self.memory = nn.Parameter(torch.randn([self.L, self.num_blocks * self.num_tokens, self.d]))\n        self.memory.requires_grad = False  # 추론 시 학습 비활성화\n        \n        # 4. 초기화 상태 추적 (Buffer로 저장)\n        self.register_buffer(\"initialized\", torch.tensor(0, dtype=torch.uint8))\n        \n        # 5. 위치 임베딩\n        self.new_memory_positional_emb = nn.Parameter(torch.zeros([1, 1, self.d]))\n        \n        # 6. BOS 임베딩 (선택)\n        if config.add_bos_embedding:\n            self.bos_embedding = nn.Parameter(torch.randn([self.L, 1, self.d]))\n```\n\n### 핵심 속성 설명\n\n| 속성 | Shape | 설명 |\n|------|-------|------|\n| `memory` | `[L, num_blocks×num_tokens, d]` | 메인 메모리 풀. 컨텍스트 정보 저장 |\n| `initialized` | `scalar (uint8)` | 메모리 초기화 여부 (0: 미초기화, 1+: 초기화됨) |\n| `new_memory_positional_emb` | `[1, 1, d]` | 새 메모리에 추가되는 위치 임베딩 |\n| `bos_embedding` | `[L, 1, d]` | 각 레이어의 시작 토큰 임베딩 |",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 14: MemoryLLM 모델 로드 (선택적)\n\"\"\"\n실제 MemoryLLM 모델을 로드합니다.\n\n주의: 이 셀은 GPU 메모리를 많이 사용합니다 (~18GB for 8B model).\n실행하기 전에 충분한 GPU 메모리가 있는지 확인하세요.\n\n로드 옵션:\n- torch_dtype=torch.float16: 메모리 절약\n- attn_implementation=\"flash_attention_2\": 속도 향상 (선택)\n- device_map=\"auto\": 자동 디바이스 배치\n\"\"\"\n\n# GPU 메모리 확인\nif torch.cuda.is_available():\n    free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)\n    print(f\"사용 가능한 GPU 메모리: {free_memory / 1e9:.1f} GB\")\n    \n    if free_memory < 16e9:\n        print(\"⚠️  경고: GPU 메모리가 16GB 미만입니다. 모델 로딩이 실패할 수 있습니다.\")\n        LOAD_MODEL = False\n    else:\n        LOAD_MODEL = True\nelse:\n    print(\"⚠️  CUDA를 사용할 수 없습니다. CPU로 실행하면 매우 느립니다.\")\n    LOAD_MODEL = False\n\n# 모델 로드 (선택적)\nmodel = None\nif LOAD_MODEL:\n    print(\"\\n모델을 로드합니다... (약 1-2분 소요)\")\n    \n    try:\n        model = MemoryLLM.from_pretrained(\n            MODEL_PATH,\n            torch_dtype=torch.float16,  # 메모리 절약\n            # attn_implementation=\"flash_attention_2\",  # Flash Attention (선택)\n            device_map=\"auto\",  # 자동 디바이스 배치\n            trust_remote_code=True,\n        )\n        print(\"✓ 모델 로드 완료!\")\n        \n        # 모델 정보 출력\n        print(f\"\\n=== 모델 정보 ===\")\n        print(f\"모델 타입: {type(model).__name__}\")\n        print(f\"num_hidden_layers (L): {model.L}\")\n        print(f\"hidden_size (d): {model.d}\")\n        print(f\"num_blocks: {model.num_blocks}\")\n        print(f\"num_tokens: {model.num_tokens}\")\n        print(f\"Memory Pool Shape: {model.memory.shape}\")\n        print(f\"Initialized: {model.initialized.item()}\")\n        \n    except Exception as e:\n        print(f\"✗ 모델 로드 실패: {e}\")\n        model = None\nelse:\n    print(\"\\n모델 로딩을 건너뜁니다. (GPU 메모리 부족 또는 CUDA 미사용)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 2.4 MemoryLLM 핵심 메서드 개요\n\nMemoryLLM의 주요 메서드들을 요약합니다. 각 메서드의 상세 분석은 다음 섹션에서 다룹니다.\n\n### 메서드 요약표\n\n| 메서드 | 위치 | 역할 |\n|--------|------|------|\n| `inject_memory()` | Line 1553 | 컨텍스트를 메모리에 주입 |\n| `drop_memory()` | Line 1573 | 메모리 풀에서 일부 제거 |\n| `update_memory_with_delta_memory()` | Line 1603 | delta_memory로 메모리 풀 갱신 |\n| `cat_memory_and_hiddens()` | Line 1673 | 메모리와 hidden states 연결 |\n| `forward()` | Line 1747 | 전체 forward pass |\n\n### 메서드 호출 흐름\n\n```\n사용자 코드:\n    model.inject_memory(context_ids, update_memory=True)\n    output = model.generate(question_ids)\n\n내부 호출 흐름:\n    inject_memory()\n        └── forward(is_injection=True, output_delta_memory=True)\n                └── cat_memory_and_hiddens() (각 레이어에서)\n                └── delta_memory 추출\n        └── update_memory_with_delta_memory()\n                └── drop_memory()\n                └── torch.cat([old_memory, delta_memory])\n    \n    generate()\n        └── forward(is_injection=False)\n                └── cat_memory_and_hiddens() (메모리 포함)\n                └── 토큰 생성\n```\n\n### 중요 플래그\n\n| 플래그 | 기본값 | 설명 |\n|--------|--------|------|\n| `is_injection` | False | True: 메모리 주입 모드, False: 생성 모드 |\n| `output_delta_memory` | False | True: delta_memory 반환 |\n| `update_memory` | False | True: 메모리 풀 실제 갱신 |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Phase 2 완료!\n\n이 섹션에서 다룬 내용:\n- Memory Pool 구조 상세 분석 (`[L, num_blocks × num_tokens, hidden_size]`)\n- MemoryLLMConfig 파라미터 이해\n- MemoryLLM 클래스 초기화 과정\n- 핵심 메서드 개요\n\n다음 섹션에서는 **Memory Operations** (inject_memory, update_memory 등)를 상세히 분석합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 3: Memory Operations - Injection & Update\n\n이 섹션에서는 MemoryLLM의 핵심 메모리 연산을 상세히 분석합니다.\n\n## 3.1 inject_memory() 메서드 분석\n\n`inject_memory()`는 새로운 컨텍스트를 메모리에 주입하는 핵심 메서드입니다.\n\n### 메서드 시그니처 (Line 1553-1570)\n\n```python\ndef inject_memory(self, context_ids, \n                  context_attention_mask=None,\n                  delta_memory=None,\n                  update_memory=False):\n    \"\"\"\n    컨텍스트를 메모리에 주입합니다.\n    \n    Args:\n        context_ids: 주입할 컨텍스트의 토큰 ID [batch, seq_len]\n        context_attention_mask: 어텐션 마스크 [batch, seq_len]\n        delta_memory: 기존에 계산된 delta_memory (다중 컨텍스트 처리용)\n        update_memory: True면 메모리 풀을 실제로 갱신\n    \n    Returns:\n        delta_memory: 추출된 메모리 표현 [batch, L, num_tokens, d]\n    \"\"\"\n    # Forward pass with injection mode\n    output = self(\n        input_ids=context_ids,\n        attention_mask=context_attention_mask,\n        delta_memory=delta_memory,\n        is_injection=True,           # 주입 모드 활성화\n        output_delta_memory=True,    # delta_memory 출력\n        return_dict=True\n    )\n    \n    # 메모리 업데이트 (선택)\n    if update_memory:\n        self.update_memory_with_delta_memory(output.delta_memory)\n    \n    return output.delta_memory\n```\n\n### 동작 과정\n\n```\n입력: context_ids [batch, seq_len]\n      예: \"Last week, John had a picnic with David...\" (30 tokens)\n\n      ↓ forward(is_injection=True)\n\n각 레이어에서:\n    1. hidden_states 계산\n    2. 마지막 num_tokens (256) 추출 → delta_memory[layer_idx]\n\n      ↓ stack across layers\n\n출력: delta_memory [batch, L, num_tokens, d]\n      예: [1, 32, 256, 4096]\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 18: inject_memory 사용 예시 (시뮬레이션)\n\"\"\"\ninject_memory()의 동작을 시뮬레이션합니다.\n\n실제 모델 없이도 메모리 주입 과정을 이해할 수 있도록\n간단한 예시를 제공합니다.\n\"\"\"\n\ndef simulate_inject_memory(context_length, num_layers=32, num_tokens=256, hidden_size=4096):\n    \"\"\"\n    inject_memory 동작을 시뮬레이션합니다.\n    \n    Args:\n        context_length: 입력 컨텍스트의 토큰 수\n        num_layers: Transformer 레이어 수\n        num_tokens: 추출할 토큰 수\n        hidden_size: 은닉층 차원\n    \n    Returns:\n        delta_memory shape 정보\n    \"\"\"\n    print(f\"=== inject_memory 시뮬레이션 ===\\n\")\n    \n    # 입력\n    print(f\"[입력]\")\n    print(f\"  context_length: {context_length} tokens\")\n    print(f\"  → 예: 'Last week, John had a picnic...' ({context_length} tokens)\")\n    print()\n    \n    # Forward pass\n    print(f\"[Forward Pass]\")\n    print(f\"  is_injection=True\")\n    print(f\"  output_delta_memory=True\")\n    print()\n    \n    # 각 레이어에서 delta_memory 추출\n    print(f\"[각 레이어에서 delta_memory 추출]\")\n    for i in [0, 1, 2, \"...\", num_layers-1]:\n        if i == \"...\":\n            print(f\"  ...\")\n        else:\n            print(f\"  Layer {i}: hidden_states[:, -{num_tokens}:, :] → [{1}, {num_tokens}, {hidden_size}]\")\n    print()\n    \n    # 최종 출력\n    print(f\"[출력: delta_memory]\")\n    print(f\"  Shape: [batch=1, L={num_layers}, num_tokens={num_tokens}, d={hidden_size}]\")\n    print(f\"  메모리 크기: {1 * num_layers * num_tokens * hidden_size * 2 / 1e6:.1f} MB (float16)\")\n    print()\n    \n    # 핵심 인사이트\n    print(f\"[핵심 인사이트]\")\n    if context_length < num_tokens:\n        print(f\"  ⚠️  context_length ({context_length}) < num_tokens ({num_tokens})\")\n        print(f\"     → 컨텍스트가 num_tokens보다 작으면 패딩 또는 반복됨\")\n    else:\n        print(f\"  ✓ context_length ({context_length}) >= num_tokens ({num_tokens})\")\n        print(f\"     → 마지막 {num_tokens} 토큰의 hidden states가 delta_memory가 됨\")\n\n# 시뮬레이션 실행\nsimulate_inject_memory(context_length=30)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\nsimulate_inject_memory(context_length=512)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.2 update_memory_with_delta_memory() 분석\n\n`update_memory_with_delta_memory()`는 추출된 delta_memory로 메모리 풀을 갱신합니다.\n\n### 메서드 로직 (Line 1603-1671)\n\n```python\ndef update_memory_with_delta_memory(self, delta_memory):\n    # Shape 정리: [batch, L, num_tokens, d] → [L, num_tokens, d]\n    if len(delta_memory.shape) == 4:\n        delta_memory = delta_memory.detach()[0]\n    \n    # === 경우 1: 첫 번째 초기화 (initialized == 0) ===\n    if self.initialized == 0:\n        # delta_memory가 메모리 풀 크기보다 작으면 반복해서 채움\n        if delta_memory.shape[1] < (self.num_tokens * self.num_blocks):\n            delta_memory = torch.cat([delta_memory] * k, dim=1)  # k번 반복\n        else:\n            # 또는 마지막 부분만 사용\n            delta_memory = delta_memory[:, -self.num_tokens * self.num_blocks:]\n        \n        self.memory.data = delta_memory  # 전체 교체\n    \n    # === 경우 2: 이후 업데이트 (initialized > 0) ===\n    else:\n        # 기존 메모리에서 일부 드롭\n        current_memory = self.drop_memory(self.memory.data.detach())\n        \n        # 새 메모리 추가\n        self.memory.data = torch.cat([current_memory, delta_memory], dim=1)\n    \n    # 초기화 플래그 증가\n    if not self.initialized:\n        self.initialized += 1\n```\n\n### 두 가지 경우 시각화\n\n```\n=== 경우 1: 첫 번째 초기화 (initialized == 0) ===\n\ndelta_memory: [████████]  (num_tokens = 256)\n                   ↓ 반복\nMemory Pool:  [████████████████████████████████████████]  (12800 tokens)\n              ^--- delta_memory로 전체 채움 ---^\n\n\n=== 경우 2: 이후 업데이트 (initialized > 0) ===\n\n기존 메모리:  [████████████████████████████████████████]  (12800 tokens)\n                   ↓ drop_memory() (1/50 = 256 tokens 제거)\n드롭 후:      [██████████████████████████████████████░░]  (12544 tokens)\n                   ↓ + delta_memory\n새 메모리:    [██████████████████████████████████████████]  (12800 tokens)\n                                                ^^^^\n                                            새로 추가된 부분\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 3.3 drop_memory() 분석\n\n`drop_memory()`는 메모리 풀에서 공간을 확보하기 위해 일부를 제거합니다.\n\n### 메서드 로직 (Line 1573-1601)\n\n```python\ndef drop_memory(self, current_memory, drop_length=None, unsequeezed=True):\n    \"\"\"\n    메모리 풀에서 일부를 랜덤하게 제거합니다.\n    \n    Args:\n        current_memory: 현재 메모리 [L, total_tokens, d]\n        drop_length: 제거할 토큰 수 (기본: 1/num_blocks)\n        unsequeezed: True면 [L, tokens, d], False면 [tokens, d]\n    \n    Returns:\n        드롭된 메모리 [L, total_tokens - drop_length, d]\n    \"\"\"\n    if unsequeezed:\n        # 드롭할 길이 계산 (기본: 1/num_blocks = 2%)\n        if drop_length is None:\n            drop_length = int(current_memory.shape[1] * (1 / self.num_blocks))\n        \n        # 남길 인덱스 랜덤 선택\n        left_indices = torch.randperm(current_memory.shape[1])[\n            :current_memory.shape[1] - drop_length\n        ]\n        \n        # 인덱스 정렬 (순서 유지)\n        left_indices = left_indices.sort()[0]\n        \n        # 선택된 인덱스만 유지\n        current_memory = current_memory[:, left_indices, :]\n        \n        return current_memory\n```\n\n### 핵심 특징\n\n1. **랜덤 드롭**: 가장 오래된 메모리가 아닌 **랜덤하게** 제거\n   - 이유: 오래된 정보도 여전히 중요할 수 있음\n   - 효과: 메모리의 다양성 유지\n\n2. **인덱스 정렬**: 드롭 후에도 남은 인덱스는 오름차순 유지\n   - 이유: 상대적 순서 보존\n   - 효과: 위치 정보의 일관성\n\n3. **레이어 공유 드롭** (기본): 모든 레이어에서 동일한 인덱스 드롭\n   - `drop_memory_per_layer=False` (기본)\n   - `drop_memory_per_layer=True`: 레이어별 독립적 드롭",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 21: drop_memory 동작 시각화\n\"\"\"\ndrop_memory()의 랜덤 드롭 동작을 시각화합니다.\n\n핵심 포인트:\n- 랜덤하게 선택 → 정렬 → 순서 유지\n- 새 메모리는 항상 끝에 추가\n\"\"\"\n\ndef visualize_drop_memory(num_blocks=50, num_tokens=256):\n    \"\"\"drop_memory 동작을 시각화합니다.\"\"\"\n    \n    total_tokens = num_blocks * num_tokens\n    drop_length = total_tokens // num_blocks  # 1/num_blocks\n    \n    # 시뮬레이션: 랜덤 인덱스 선택\n    np.random.seed(42)  # 재현성을 위해\n    all_indices = np.arange(total_tokens)\n    \n    # 드롭할 인덱스 (랜덤)\n    drop_indices = np.random.choice(all_indices, drop_length, replace=False)\n    drop_indices.sort()\n    \n    # 남길 인덱스\n    keep_indices = np.setdiff1d(all_indices, drop_indices)\n    \n    # 시각화\n    fig, axes = plt.subplots(3, 1, figsize=(14, 8))\n    \n    # 1. 원본 메모리\n    ax1 = axes[0]\n    ax1.barh(0, total_tokens, height=0.6, color='lightblue', edgecolor='navy')\n    ax1.set_xlim(0, total_tokens + 500)\n    ax1.set_ylim(-0.5, 0.5)\n    ax1.set_yticks([])\n    ax1.set_title(f'1. 원본 메모리: {total_tokens:,} tokens ({num_blocks} blocks × {num_tokens} tokens)')\n    ax1.axvline(x=total_tokens - num_tokens, color='red', linestyle='--', label='New memory position')\n    \n    # 2. 드롭된 위치 표시\n    ax2 = axes[1]\n    ax2.barh(0, total_tokens, height=0.6, color='lightblue', edgecolor='navy', alpha=0.5)\n    # 드롭 위치 표시 (샘플링하여 표시)\n    sample_drops = drop_indices[::10]  # 10개마다 표시\n    for idx in sample_drops:\n        ax2.axvline(x=idx, color='red', alpha=0.3, linewidth=1)\n    ax2.set_xlim(0, total_tokens + 500)\n    ax2.set_ylim(-0.5, 0.5)\n    ax2.set_yticks([])\n    ax2.set_title(f'2. 랜덤 드롭: {drop_length} tokens 제거 (빨간 선 = 드롭 위치, 샘플)')\n    \n    # 3. 드롭 후 + 새 메모리\n    ax3 = axes[2]\n    remaining = total_tokens - drop_length\n    ax3.barh(0, remaining, height=0.6, color='lightblue', edgecolor='navy', label=f'기존 메모리 ({remaining:,})')\n    ax3.barh(0, num_tokens, left=remaining, height=0.6, color='coral', edgecolor='darkred', label=f'새 메모리 ({num_tokens})')\n    ax3.set_xlim(0, total_tokens + 500)\n    ax3.set_ylim(-0.5, 0.5)\n    ax3.set_yticks([])\n    ax3.set_title(f'3. 드롭 후 + 새 메모리: {remaining:,} + {num_tokens} = {remaining + num_tokens:,} tokens')\n    ax3.legend(loc='upper right')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 통계 출력\n    print(f\"\\n=== drop_memory 통계 ===\")\n    print(f\"원본 메모리: {total_tokens:,} tokens\")\n    print(f\"드롭 비율: 1/{num_blocks} = {100/num_blocks:.1f}%\")\n    print(f\"드롭 토큰 수: {drop_length:,} tokens\")\n    print(f\"남은 토큰 수: {remaining:,} tokens\")\n    print(f\"새 메모리 추가: {num_tokens} tokens\")\n    print(f\"최종 메모리: {remaining + num_tokens:,} tokens (원본과 동일)\")\n\nvisualize_drop_memory()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 3.4 cat_memory_and_hiddens() 분석\n\n`cat_memory_and_hiddens()`는 각 레이어에서 메모리와 현재 hidden states를 연결합니다.\n\n### 메서드 로직 (Line 1673-1745)\n\n```python\ndef cat_memory_and_hiddens(self, idx, hidden_states, delta_memory=None, \n                           is_injection=False, cat_to_maximum_memory=False):\n    \"\"\"\n    메모리와 hidden states를 연결합니다.\n    \n    Args:\n        idx: 현재 레이어 인덱스\n        hidden_states: 현재 레이어의 입력 [batch, seq_len, d]\n        delta_memory: 이전에 계산된 delta_memory (선택)\n        is_injection: 주입 모드 여부\n    \n    Returns:\n        연결된 hidden states [batch, memory_len + seq_len, d]\n    \"\"\"\n    # 메모리가 초기화되지 않았으면 그냥 반환\n    if not self.initialized:\n        return hidden_states\n    \n    # === 경우 1: delta_memory가 없는 경우 ===\n    if delta_memory is None or len(delta_memory) == 0:\n        if is_injection:\n            # 주입 모드: 마지막 num_tokens만 사용\n            cur_memory = self.memory[idx][-self.num_tokens:]  # [num_tokens, d]\n        else:\n            # 생성 모드: 전체 메모리 사용\n            cur_memory = self.memory[idx]  # [total_tokens, d]\n    \n    # === 경우 2: delta_memory가 있는 경우 ===\n    else:\n        cur_memory = delta_memory[:, idx]  # [batch, num_tokens, d]\n        \n        if not is_injection:\n            # 생성 모드에서는 old_memory도 샘플링하여 연결\n            old_memory = self.memory[idx].detach()\n            sampled_indices = torch.randperm(old_memory.shape[0])[\n                :old_memory.shape[0] - cur_memory.shape[1]\n            ].sort()[0]\n            old_memory = old_memory[sampled_indices]\n            cur_memory = torch.cat([old_memory, cur_memory], dim=1)\n    \n    # BOS 임베딩 추가 (선택)\n    if self.add_bos_embedding:\n        cur_memory = torch.cat([self.bos_embedding[idx], cur_memory], dim=1)\n    \n    # 메모리 + hidden_states 연결\n    return torch.cat([cur_memory, hidden_states], dim=1)\n```\n\n### 동작 시각화\n\n```\n=== is_injection=True (주입 모드) ===\n\ncur_memory:    [████]              (최근 num_tokens = 256)\nhidden_states:        [██████████] (입력 seq_len = 30)\n                   ↓ cat\n결과:          [████][██████████]  (256 + 30 = 286)\n\n\n=== is_injection=False (생성 모드) ===\n\ncur_memory:    [████████████████████████████████████████] (전체 12800)\nhidden_states:                                           [██] (질문 10)\n                   ↓ cat\n결과:          [████████████████████████████████████████][██] (12800 + 10 = 12810)\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 23: 메모리 연산 종합 예제 (실제 모델 사용 시)\n\"\"\"\n실제 모델이 로드된 경우 메모리 연산을 수행합니다.\n\n이 셀은 다음을 시연합니다:\n1. inject_memory()로 컨텍스트 주입\n2. 메모리 상태 확인\n3. 추가 컨텍스트 주입\n4. 메모리 변화 관찰\n\"\"\"\n\ndef demonstrate_memory_operations(model, tokenizer):\n    \"\"\"메모리 연산을 시연합니다.\"\"\"\n    \n    print(\"=== 메모리 연산 시연 ===\\n\")\n    \n    # 1. 초기 상태 확인\n    print(\"[1. 초기 상태]\")\n    print(f\"  initialized: {model.initialized.item()}\")\n    print(f\"  memory shape: {model.memory.shape}\")\n    print()\n    \n    # 2. 첫 번째 컨텍스트 주입\n    context1 = \"John's favorite color is blue. He always wears blue shirts and has a blue car.\"\n    print(f\"[2. 첫 번째 컨텍스트 주입]\")\n    print(f\"  컨텍스트: '{context1[:50]}...'\")\n    \n    context1_ids = tokenizer(context1, return_tensors='pt').input_ids.to(model.device)\n    print(f\"  토큰 수: {context1_ids.shape[1]}\")\n    \n    # 메모리 백업\n    memory_before = model.memory.data.clone()\n    \n    # 주입\n    delta_memory1 = model.inject_memory(context1_ids, update_memory=True)\n    \n    print(f\"  delta_memory shape: {delta_memory1.shape}\")\n    print(f\"  initialized after: {model.initialized.item()}\")\n    \n    # 메모리 변화 확인\n    memory_after = model.memory.data\n    memory_diff = (memory_after - memory_before).abs().mean().item()\n    print(f\"  메모리 변화량 (mean abs diff): {memory_diff:.6f}\")\n    print()\n    \n    # 3. 두 번째 컨텍스트 주입\n    context2 = \"Mary loves strawberries. She eats strawberries every morning for breakfast.\"\n    print(f\"[3. 두 번째 컨텍스트 주입]\")\n    print(f\"  컨텍스트: '{context2[:50]}...'\")\n    \n    context2_ids = tokenizer(context2, return_tensors='pt').input_ids.to(model.device)\n    print(f\"  토큰 수: {context2_ids.shape[1]}\")\n    \n    memory_before2 = model.memory.data.clone()\n    \n    delta_memory2 = model.inject_memory(context2_ids, update_memory=True)\n    \n    memory_after2 = model.memory.data\n    memory_diff2 = (memory_after2 - memory_before2).abs().mean().item()\n    print(f\"  메모리 변화량: {memory_diff2:.6f}\")\n    print()\n    \n    # 4. 최종 상태\n    print(f\"[4. 최종 상태]\")\n    print(f\"  총 주입 횟수: 2\")\n    print(f\"  메모리 shape: {model.memory.shape}\")\n    print(f\"  각 주입으로 인한 메모리 갱신:\")\n    print(f\"    - drop: 1/{model.num_blocks} = {model.num_tokens} tokens\")\n    print(f\"    - add: {model.num_tokens} tokens (delta_memory)\")\n    \n    return delta_memory1, delta_memory2\n\n# 실행 (모델이 로드된 경우에만)\nif model is not None and tokenizer is not None:\n    delta1, delta2 = demonstrate_memory_operations(model, tokenizer)\nelse:\n    print(\"모델이 로드되지 않았습니다.\")\n    print(\"위의 '모델 로드' 셀을 먼저 실행하거나,\")\n    print(\"코드를 참고하여 메모리 연산 흐름을 이해하세요.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Phase 3 완료!\n\n이 섹션에서 다룬 내용:\n- `inject_memory()`: 컨텍스트를 메모리에 주입\n- `update_memory_with_delta_memory()`: delta_memory로 메모리 풀 갱신\n- `drop_memory()`: 랜덤 드롭으로 공간 확보\n- `cat_memory_and_hiddens()`: 메모리와 hidden states 연결\n\n### 핵심 정리\n\n| 메서드 | 역할 | 입력 | 출력 |\n|--------|------|------|------|\n| `inject_memory` | 컨텍스트 주입 | context_ids | delta_memory |\n| `update_memory_with_delta_memory` | 메모리 갱신 | delta_memory | None (in-place) |\n| `drop_memory` | 공간 확보 | current_memory | dropped_memory |\n| `cat_memory_and_hiddens` | 연결 | hidden_states | [memory, hidden_states] |\n\n다음 섹션에서는 **Forward Pass & Generation**을 상세히 분석합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 4: Forward Pass & Generation\n\n이 섹션에서는 MemoryLLM의 forward pass와 텍스트 생성을 상세히 분석합니다.\n\n## 4.1 forward() 메서드 시그니처\n\n`forward()` 메서드는 MemoryLLM의 핵심 연산을 수행합니다.\n\n### 메서드 시그니처 (Line 1747-1764)\n\n```python\ndef forward(\n    self,\n    input_ids: torch.LongTensor = None,           # 입력 토큰 ID\n    attention_mask: Optional[torch.Tensor] = None, # 어텐션 마스크\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_values: Optional[List[torch.FloatTensor]] = None,  # KV 캐시\n    inputs_embeds: Optional[torch.FloatTensor] = None,\n    delta_memory: Optional[List[List[torch.FloatTensor]]] = None,  # 메모리 (MemoryLLM 추가)\n    labels: torch.LongTensor = None,              # 학습용 레이블\n    use_cache: Optional[bool] = None,\n    output_attentions: Optional[bool] = None,\n    output_delta_memory: Optional[bool] = None,   # delta_memory 출력 (MemoryLLM 추가)\n    output_hidden_states: Optional[bool] = None,\n    return_dict: Optional[bool] = None,\n    cache_position: Optional[torch.LongTensor] = None,\n    is_injection: Optional[bool] = None,          # 주입 모드 (MemoryLLM 추가)\n    cat_to_maximum_memory: Optional[bool] = False,\n) -> Union[Tuple, MemoryLMOutputWithPastAndCrossAttentions]:\n```\n\n### MemoryLLM 추가 파라미터\n\n| 파라미터 | 타입 | 설명 |\n|---------|------|------|\n| `delta_memory` | `[batch, L, num_tokens, d]` | 이전에 계산된 메모리 (다중 컨텍스트 처리) |\n| `output_delta_memory` | `bool` | True면 delta_memory 반환 |\n| `is_injection` | `bool` | True: 주입 모드, False: 생성 모드 |\n| `cat_to_maximum_memory` | `bool` | 최대 메모리 크기로 연결 |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 4.2 Forward Pass 단계별 분석\n\n### 전체 흐름\n\n```\n입력: input_ids [batch, seq_len]\n       ↓\n1. Embedding: inputs_embeds [batch, seq_len, d]\n       ↓\n2. Cache Position 계산 (메모리 고려)\n       ↓\n3. 각 Decoder Layer 순회:\n   for idx in range(num_layers):\n       a. cat_memory_and_hiddens() → [batch, memory_len + seq_len, d]\n       b. new_memory_positional_emb 추가 (is_injection일 때)\n       c. decoder_layer forward\n       d. delta_memory 추출 (output_delta_memory일 때)\n       e. 원래 seq_len으로 복원\n       ↓\n4. Final Layer Norm\n       ↓\n5. LM Head → logits [batch, seq_len, vocab_size]\n       ↓\n6. Loss 계산 (labels가 있으면)\n       ↓\n출력: MemoryLMOutputWithPastAndCrossAttentions\n      - loss, logits, delta_memory, past_key_values, ...\n```\n\n### 핵심 로직: Cache Position 계산\n\n```python\n# 메모리가 초기화된 경우\nif self.initialized:\n    if is_injection:\n        # 주입 모드: 최근 메모리 + 입력\n        cache_position = torch.arange(\n            0, inputs_embeds.shape[1] + self.num_tokens + int(self.add_bos_embedding)\n        )\n        # 예: [0, 1, 2, ..., 256 + 30 + 1] = 287 positions\n    else:\n        # 생성 모드: 전체 메모리 + 입력\n        cache_position = torch.arange(\n            0, inputs_embeds.shape[1] + self.num_tokens * self.num_blocks + int(self.add_bos_embedding)\n        )\n        # 예: [0, 1, 2, ..., 12800 + 10 + 1] = 12811 positions\n```\n\n### RoPE (Rotary Position Embedding) 처리\n\nMemoryLLM은 긴 position을 처리하기 위해 RoPE scaling을 사용합니다:\n\n```python\n# config에서 설정\nif rope_scaling is not None:\n    rope_scaling['factor'] = (num_memory_tokens + max_length) / max_position_embeddings\n    # 예: (12800 + 2048) / 4096 ≈ 3.6x scaling\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 27: Forward Pass 시뮬레이션\n\"\"\"\nForward Pass의 각 단계를 시뮬레이션합니다.\n\n실제 연산 없이 shape 변화를 추적하여 흐름을 이해합니다.\n\"\"\"\n\ndef simulate_forward_pass(seq_len, num_layers=32, num_blocks=50, num_tokens=256, \n                          hidden_size=4096, is_injection=True, initialized=True):\n    \"\"\"\n    Forward pass를 시뮬레이션합니다.\n    \n    Args:\n        seq_len: 입력 시퀀스 길이\n        num_layers: 레이어 수\n        num_blocks: 메모리 블록 수\n        num_tokens: 블록당 토큰 수\n        hidden_size: 은닉층 차원\n        is_injection: 주입 모드 여부\n        initialized: 메모리 초기화 여부\n    \"\"\"\n    mode = \"주입 모드 (is_injection=True)\" if is_injection else \"생성 모드 (is_injection=False)\"\n    print(f\"=== Forward Pass 시뮬레이션: {mode} ===\\n\")\n    \n    # 1. 입력\n    print(f\"[1. 입력]\")\n    print(f\"  input_ids shape: [1, {seq_len}]\")\n    print()\n    \n    # 2. Embedding\n    print(f\"[2. Embedding]\")\n    print(f\"  inputs_embeds shape: [1, {seq_len}, {hidden_size}]\")\n    print()\n    \n    # 3. Cache Position 계산\n    print(f\"[3. Cache Position 계산]\")\n    if not initialized:\n        cache_len = seq_len\n        print(f\"  (메모리 미초기화)\")\n    elif is_injection:\n        # 주입 모드: 최근 메모리 + 입력 + BOS\n        memory_len = num_tokens\n        cache_len = memory_len + seq_len + 1  # +1 for BOS\n        print(f\"  memory_len (recent): {memory_len}\")\n    else:\n        # 생성 모드: 전체 메모리 + 입력 + BOS\n        memory_len = num_blocks * num_tokens\n        cache_len = memory_len + seq_len + 1  # +1 for BOS\n        print(f\"  memory_len (full): {memory_len}\")\n    \n    print(f\"  cache_position: [0, 1, ..., {cache_len-1}] (length: {cache_len})\")\n    print()\n    \n    # 4. Decoder Layers\n    print(f\"[4. Decoder Layers]\")\n    for layer_idx in [0, 1, \"...\", num_layers-1]:\n        if layer_idx == \"...\":\n            print(f\"  ...\")\n            continue\n            \n        print(f\"  Layer {layer_idx}:\")\n        \n        # cat_memory_and_hiddens\n        if initialized:\n            if is_injection:\n                concat_len = num_tokens + seq_len\n            else:\n                concat_len = num_blocks * num_tokens + seq_len\n            print(f\"    a. cat_memory_and_hiddens → [1, {concat_len}, {hidden_size}]\")\n            \n            if is_injection:\n                print(f\"    b. + new_memory_positional_emb (마지막 {num_tokens} tokens)\")\n        else:\n            concat_len = seq_len\n            print(f\"    a. 메모리 없음 → [1, {seq_len}, {hidden_size}]\")\n        \n        # decoder_layer forward\n        print(f\"    c. decoder_layer forward\")\n        \n        # delta_memory 추출\n        if is_injection:\n            print(f\"    d. delta_memory[{layer_idx}] = hidden_states[:, -{num_tokens}:, :]\")\n        \n        # 복원\n        print(f\"    e. hidden_states = hidden_states[:, -{seq_len}:, :] → [1, {seq_len}, {hidden_size}]\")\n        print()\n    \n    # 5. LM Head\n    vocab_size = 32000\n    print(f\"[5. LM Head]\")\n    print(f\"  logits shape: [1, {seq_len}, {vocab_size}]\")\n    print()\n    \n    # 6. 출력\n    print(f\"[6. 출력]\")\n    print(f\"  logits: [1, {seq_len}, {vocab_size}]\")\n    if is_injection:\n        print(f\"  delta_memory: [1, {num_layers}, {num_tokens}, {hidden_size}]\")\n\n# 주입 모드 시뮬레이션\nsimulate_forward_pass(seq_len=30, is_injection=True)\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 생성 모드 시뮬레이션\nsimulate_forward_pass(seq_len=10, is_injection=False)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4.3 텍스트 생성 (Generation)\n\nMemoryLLM에서 텍스트 생성은 HuggingFace의 `generate()` 메서드를 그대로 사용합니다.\n\n### 기본 사용법\n\n```python\n# 1. 컨텍스트 주입\ncontext = \"John's favorite fruit is apple. He eats apples every day.\"\ncontext_ids = tokenizer(context, return_tensors='pt').input_ids.cuda()\nmodel.inject_memory(context_ids, update_memory=True)\n\n# 2. 질문 생성\nquestion = \"What is John's favorite fruit?\"\nquestion_ids = tokenizer(question, return_tensors='pt').input_ids.cuda()\n\n# 3. 응답 생성\noutput = model.generate(\n    inputs=question_ids,\n    max_new_tokens=20,\n    pad_token_id=tokenizer.pad_token_id,\n    eos_token_id=tokenizer.eos_token_id,\n)\n\n# 4. 디코딩\nresponse = tokenizer.decode(output[0], skip_special_tokens=True)\nprint(response)  # \"What is John's favorite fruit? Apple.\"\n```\n\n### 생성 파라미터\n\n| 파라미터 | 설명 | 권장값 |\n|---------|------|--------|\n| `max_new_tokens` | 생성할 최대 토큰 수 | 10-50 (QA), 100-500 (긴 응답) |\n| `temperature` | 샘플링 온도 | 0.7-1.0 |\n| `top_p` | Nucleus sampling | 0.9-0.95 |\n| `do_sample` | 샘플링 사용 여부 | True (다양성), False (결정적) |\n| `num_beams` | Beam search | 1-5 |\n\n### 생성 시 메모리 활용\n\n```\n생성 모드에서 각 레이어:\n    hidden_states = cat_memory_and_hiddens(idx, hidden_states)\n    \n    결과: [전체 메모리 (12800 tokens)] + [질문 (10 tokens)]\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n         이 메모리 덕분에 주입된 컨텍스트 정보 활용 가능\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 29: 전체 추론 파이프라인 예제\n\"\"\"\nMemoryLLM의 전체 추론 파이프라인을 시연합니다.\n\n흐름:\n1. 컨텍스트 주입 (inject_memory)\n2. 질문 생성 (generate)\n3. 추가 컨텍스트 주입\n4. 다시 질문 (지식 유지 확인)\n\"\"\"\n\ndef run_inference_pipeline(model, tokenizer):\n    \"\"\"전체 추론 파이프라인을 실행합니다.\"\"\"\n    \n    print(\"=== MemoryLLM 추론 파이프라인 ===\\n\")\n    \n    # 메모리 초기 상태 백업\n    original_memory = model.memory.data.detach().cpu().clone()\n    original_initialized = model.initialized.item()\n    \n    try:\n        # 1. 첫 번째 컨텍스트 주입\n        context1 = \"John's favorite color is blue. He paints his room blue and drives a blue car.\"\n        print(f\"[1. 첫 번째 컨텍스트 주입]\")\n        print(f\"  컨텍스트: '{context1}'\")\n        \n        context1_ids = tokenizer(context1, return_tensors='pt').input_ids.to(model.device)\n        model.inject_memory(context1_ids, update_memory=True)\n        print(f\"  ✓ 메모리 주입 완료\\n\")\n        \n        # 2. 첫 번째 질문\n        question1 = \"What is John's favorite color?\"\n        print(f\"[2. 첫 번째 질문]\")\n        print(f\"  질문: '{question1}'\")\n        \n        q1_ids = tokenizer(question1, return_tensors='pt').input_ids.to(model.device)\n        output1 = model.generate(\n            inputs=q1_ids,\n            max_new_tokens=20,\n            pad_token_id=tokenizer.pad_token_id,\n            do_sample=False,  # 결정적 생성\n        )\n        response1 = tokenizer.decode(output1[0], skip_special_tokens=True)\n        print(f\"  응답: '{response1}'\\n\")\n        \n        # 3. 두 번째 컨텍스트 주입 (무관한 정보)\n        context2 = \"Mary loves strawberries. She grows strawberries in her garden.\"\n        print(f\"[3. 두 번째 컨텍스트 주입 (무관한 정보)]\")\n        print(f\"  컨텍스트: '{context2}'\")\n        \n        context2_ids = tokenizer(context2, return_tensors='pt').input_ids.to(model.device)\n        model.inject_memory(context2_ids, update_memory=True)\n        print(f\"  ✓ 메모리 주입 완료 (기존 메모리 일부 드롭)\\n\")\n        \n        # 4. 다시 첫 번째 질문 (지식 유지 확인)\n        print(f\"[4. 다시 첫 번째 질문 (지식 유지 확인)]\")\n        print(f\"  질문: '{question1}'\")\n        \n        output2 = model.generate(\n            inputs=q1_ids,\n            max_new_tokens=20,\n            pad_token_id=tokenizer.pad_token_id,\n            do_sample=False,\n        )\n        response2 = tokenizer.decode(output2[0], skip_special_tokens=True)\n        print(f\"  응답: '{response2}'\\n\")\n        \n        # 5. 결과 분석\n        print(f\"[5. 결과 분석]\")\n        print(f\"  첫 번째 응답: '{response1}'\")\n        print(f\"  두 번째 응답: '{response2}'\")\n        \n        # 'blue'가 응답에 포함되는지 확인\n        if 'blue' in response1.lower() and 'blue' in response2.lower():\n            print(f\"  ✓ 지식 유지 성공! 'blue' 정보가 유지됨\")\n        elif 'blue' in response1.lower():\n            print(f\"  △ 부분적 지식 유지: 첫 응답에만 'blue' 포함\")\n        else:\n            print(f\"  ✗ 지식 손실 가능성\")\n            \n    finally:\n        # 메모리 복원\n        model.memory.data = original_memory.to(model.device)\n        model.initialized.fill_(original_initialized)\n        print(f\"\\n[메모리 원래 상태로 복원됨]\")\n\n# 실행 (모델이 로드된 경우에만)\nif model is not None and tokenizer is not None:\n    run_inference_pipeline(model, tokenizer)\nelse:\n    print(\"모델이 로드되지 않았습니다.\")\n    print(\"\\n위 코드는 다음 흐름을 시연합니다:\")\n    print(\"1. 컨텍스트 주입 → 질문 → 응답 확인\")\n    print(\"2. 추가 컨텍스트 주입 → 같은 질문 → 지식 유지 확인\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Phase 4 완료!\n\n이 섹션에서 다룬 내용:\n- `forward()` 메서드 시그니처 및 MemoryLLM 추가 파라미터\n- Forward pass 단계별 분석\n- Cache position 계산 로직\n- 텍스트 생성 (`generate()`) 사용법\n- 전체 추론 파이프라인 예제\n\n### Inference 핵심 정리\n\n```\n전체 Inference 흐름:\n\n1. inject_memory(context_ids, update_memory=True)\n   → delta_memory 추출 → 메모리 풀 업데이트\n\n2. model.generate(question_ids)\n   → forward(is_injection=False)\n   → 전체 메모리 + 질문 연결\n   → 토큰 생성\n```\n\n다음 섹션에서는 **Training Architecture**를 분석합니다.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 5: Training Architecture\n\n이 섹션에서는 MemoryLLM의 학습 아키텍처를 분석합니다.\n\n## 5.1 PyTorch Lightning 구조\n\nMemoryLLM의 학습은 **PyTorch Lightning** 프레임워크를 기반으로 합니다.\n\n### 클래스 계층 구조\n\n```\nLlamaMemoryModelPL (train/MemoryLLM/memoryllm/models/memory.py)\n    └── BaseMemoryModelPL (train/MemoryLLM/memoryllm/models/base.py)\n            └── pl.LightningModule\n                    └── LlamaDropMemoryModel (modules/memory_llama.py)\n                            └── LlamaForCausalLM\n                                    └── BaseMemoryModel\n```\n\n### 주요 파일\n\n| 파일 | 역할 |\n|------|------|\n| `train/main.py` | 학습 엔트리 포인트, DataModule |\n| `train/MemoryLLM/memoryllm/models/memory.py` | `LlamaMemoryModelPL` 정의 |\n| `train/MemoryLLM/memoryllm/models/base.py` | `BaseMemoryModelPL` (training_step, validation_step) |\n| `train/MemoryLLM/memoryllm/modules/memory_llama.py` | `LlamaDropMemoryModel` (학습용 모델) |\n| `train/MemoryLLM/configs/` | YAML 설정 파일들 |\n\n## 5.2 YAML 설정 파일 분석\n\n학습 설정은 YAML 파일로 관리됩니다.\n\n### 예시: `llama_30x256.yaml`\n\n```yaml\nmodel:\n  base_learning_rate: 4.6e-6           # 학습률 (LoRA 사용 시 작은 값)\n  target: MemoryLLM.memoryllm.models.memory.LlamaMemoryModelPL\n  params:\n    monitor: val/avg_acc               # 검증 지표\n    num_blocks: 30                     # 메모리 블록 수\n    num_tokens: 256                    # 블록당 토큰 수\n    update_memory_during_training: true\n    \n    # LoRA 설정\n    lora_config:\n      r: 8                             # LoRA 랭크\n      lora_alpha: 32\n      lora_dropout: 0.1\n      target_modules: ['q_proj', 'v_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj']\n    \n    # 메모리 전략\n    cat_and_drop_memory: true          # 핵심: 다중 컨텍스트 학습\n    drop_memory_per_layer: true\n    \n    # 컨텍스트 스케줄\n    num_contexts_schedule:\n      checkpoints: [10000, 15000, 20000, 30000, 50000, 60000]\n      values: [1, 2, 3, 4, 5, 10, 20]  # 점진적 컨텍스트 증가\n\ndata:\n  batch_size: 1\n  train:\n    target: MemoryLLM.memoryllm.data.redpajama.RedPajamaDataset\n  validation:\n    - target: MemoryLLM.memoryllm.data.nq.NQDataset\n    - target: MemoryLLM.memoryllm.data.squad.SQuADDataset\n\nlightning:\n  trainer:\n    accelerator: gpu\n    strategy: deepspeed_stage_2        # 분산 학습\n    precision: 16-mixed                # 혼합 정밀도\n    accumulate_grad_batches: 4         # 그래디언트 누적\n```",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## 5.3 training_step() 핵심 로직\n\n`training_step()`은 MemoryLLM 학습의 핵심입니다.\n\n### cat_and_drop_memory 전략\n\n```python\ndef training_step(self, batch, batch_idx):\n    contexts_ids, contexts_attention_masks, sentence_ids, \\\n    sentence_attention_masks, labels = batch\n    \n    # 현재 step에 맞는 컨텍스트 수 결정\n    num_of_contexts = self.num_contexts(self.trainer.global_step)\n    \n    # === cat_and_drop_memory 전략 ===\n    if self.cat_and_drop_memory:\n        all_delta_memory = None\n        \n        # 여러 컨텍스트 순차 처리\n        for i in range(len(contexts_ids)):\n            output = self.model(\n                input_ids=contexts_ids[i],\n                attention_mask=contexts_attention_masks[i],\n                output_delta_memory=True,\n                is_injection=True\n            )\n            \n            delta_memory = output.delta_memory.detach()\n            \n            if all_delta_memory is None:\n                all_delta_memory = delta_memory\n            else:\n                # 기존 delta_memory에서 일부 드롭 + 새 delta_memory 추가\n                all_delta_memory = self.model.drop_memory(all_delta_memory[0]).unsqueeze(0)\n                all_delta_memory = torch.cat([all_delta_memory, delta_memory], dim=2)\n        \n        delta_memory = all_delta_memory.detach()\n    \n    # 문장에 대한 Language Modeling\n    sentence_labels = sentence_ids.clone()\n    sentence_labels[sentence_attention_masks == 0] = -100  # 패딩 무시\n    \n    output = self.model(\n        input_ids=sentence_ids,\n        attention_mask=sentence_attention_masks,\n        labels=sentence_labels,\n        delta_memory=delta_memory,\n        is_injection=False\n    )\n    \n    return output.loss\n```\n\n### num_contexts_schedule\n\n컨텍스트 수를 점진적으로 증가시킵니다:\n\n```\nStep 0 ~ 10000:     1개 컨텍스트  (단순 학습)\nStep 10000 ~ 15000: 2개 컨텍스트\nStep 15000 ~ 20000: 3개 컨텍스트\n...\nStep 60000+:        20개 컨텍스트 (복잡한 메모리 관리 학습)\n```\n\n이 전략은 **커리큘럼 학습**의 일종으로, 모델이 점진적으로 복잡한 메모리 관리를 학습합니다.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Cell 33: 학습 실행 명령어\n\"\"\"\nMemoryLLM 학습 실행 방법입니다.\n\n주의: 실제 학습에는 고사양 GPU (A100 80GB 권장)가 필요합니다.\n\"\"\"\n\nprint(\"=== MemoryLLM 학습 실행 방법 ===\\n\")\n\nprint(\"[1. 환경 준비]\")\nprint(\"  cd /path/to/MemoryLLM/train\")\nprint(\"  pip install -r ../requirements.txt\")\nprint()\n\nprint(\"[2. 데이터 준비]\")\nprint(\"  # RedPajama 데이터 다운로드\")\nprint(\"  mkdir -p data/redpajama\")\nprint(\"  # (RedPajama 데이터는 별도 다운로드 필요)\")\nprint()\nprint(\"  # NQ/SQuAD 검증 데이터\")\nprint(\"  mkdir -p data/nq data/squad\")\nprint(\"  # (데이터셋 다운로드)\")\nprint()\n\nprint(\"[3. 학습 실행]\")\nprint(\"  # Llama-2-7B 기반 학습 (30 blocks × 256 tokens)\")\nprint(\"  python main.py -t --base MemoryLLM/configs/llama/llama_30x256.yaml\")\nprint()\nprint(\"  # OpenLLaMA 디버깅용 (작은 설정)\")\nprint(\"  python main.py -t --base MemoryLLM/configs/openllama/openllama_4x256.yaml\")\nprint()\n\nprint(\"[4. 학습 모니터링]\")\nprint(\"  # TensorBoard\")\nprint(\"  tensorboard --logdir=logs/\")\nprint()\n\nprint(\"[5. 체크포인트에서 재개]\")\nprint(\"  python main.py -t --base MemoryLLM/configs/llama/llama_30x256.yaml \\\\\")\nprint(\"                 -r /path/to/checkpoint.ckpt\")\nprint()\n\nprint(\"[참고: 예상 학습 시간]\")\nprint(\"  - A100 80GB 1장: ~2-3일 (60K steps)\")\nprint(\"  - 메모리 요구량: ~40GB+ (DeepSpeed Stage 2)\")\nprint(\"  - 그래디언트 누적: 4 (effective batch size = 4)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Phase 5 & 6 완료!\n\n### Training 핵심 요약\n\n| 항목 | 설명 |\n|------|------|\n| **프레임워크** | PyTorch Lightning + DeepSpeed |\n| **학습 전략** | `cat_and_drop_memory` (다중 컨텍스트 처리) |\n| **파라미터 효율화** | LoRA (r=8, alpha=32) |\n| **커리큘럼 학습** | `num_contexts_schedule` (1→20 점진 증가) |\n| **데이터셋** | RedPajama (학습), NQ/SQuAD (검증) |\n\n---\n\n# Part 7: Evaluation & Metrics\n\n## 7.1 Knowledge Retention 평가\n\nMemoryLLM의 핵심 평가는 **지식 유지 능력**입니다.\n\n### 평가 시나리오\n\n```\n1. 관련 컨텍스트 주입 → 질문 → 정확도 측정 (Acc_0)\n2. 무관한 컨텍스트 1개 추가 → 같은 질문 → 정확도 측정 (Acc_1)\n3. 무관한 컨텍스트 2개 추가 → 같은 질문 → 정확도 측정 (Acc_2)\n...\nN. 무관한 컨텍스트 N개 추가 → 같은 질문 → 정확도 측정 (Acc_N)\n\n목표: Acc_N ≈ Acc_0 (무관한 정보가 추가되어도 지식 유지)\n```\n\n### 평가 실행\n\n```bash\nmkdir results\n\n# NQ + SQuAD 평가 (5개 무관한 컨텍스트)\npython test_qa_memory.py \\\n    --model YuWangX/memoryllm-7b \\\n    --nuc 5 \\\n    --datasets naturalqa squad \\\n    --num_samples 100\n\n# LongBench 평가\npython longbench_pred.py \\\n    --model memoryllm-7b \\\n    --datasets hotpotqa \\\n    --max_length 16384\n```\n\n## 7.2 평가 메트릭\n\n`metrics.py`에 정의된 주요 메트릭:\n\n| 메트릭 | 함수 | 용도 |\n|--------|------|------|\n| **Exact Match** | `normalize_answer()` | QA 정확도 |\n| **F1 Score** | `qa_f1_score()` | 토큰 레벨 F1 |\n| **ROUGE-L** | `rouge_score()` | 요약 평가 |\n| **Count Score** | `count_score()` | 숫자 추출 |",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# Part 8: Advanced Topics\n\n## 8.1 M+ (MPlus) 확장 모델\n\nM+는 MemoryLLM에 **Long-Term Memory (LTM)**을 추가한 확장 버전입니다.\n\n### MemoryLLM vs M+\n\n| 특성 | MemoryLLM | M+ |\n|------|-----------|------|\n| **메모리 유형** | STM only | STM + LTM |\n| **메모리 크기** | 고정 (12800 tokens) | 확장 가능 |\n| **저장 위치** | GPU | STM: GPU, LTM: CPU |\n| **검색 메커니즘** | 없음 | Modern Hopfield Network |\n\n### M+ 사용법\n\n```python\nfrom modeling_mplus import MPlus\n\nmodel = MPlus.from_pretrained(\"YuWangX/mplus-8b\", \n                               torch_dtype=torch.float16,\n                               device_map=\"auto\")\n\n# LTM 검색 활성화\nmodel.inject_memory(context_ids, update_memory=True, use_retriever=True)\n```\n\n## 8.2 Chat Model 사용법\n\nMemoryLLM은 Chat 버전도 제공합니다.\n\n```python\nmodel = MemoryLLM.from_pretrained(\"YuWangX/memoryllm-8b-chat\", ...)\ntokenizer = AutoTokenizer.from_pretrained(\"YuWangX/memoryllm-8b-chat\")\n\n# 컨텍스트 주입\nctx = \"Last week, John had a wonderful picnic with David. David loves strawberries.\"\nmodel.inject_memory(tokenizer(ctx, return_tensors='pt').input_ids.cuda(), update_memory=True)\n\n# Chat 템플릿 적용\nmessages = [{\"role\": \"user\", \"content\": \"What fruit does David like?\"}]\ninputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", \n                                        add_generation_prompt=True)[:, 1:]\n\n# 생성\noutputs = model.generate(\n    inputs.cuda(), \n    max_new_tokens=50,\n    eos_token_id=[tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]\n)\n```\n\n## 8.3 Best Practices\n\n### 메모리 관리\n\n1. **메모리 백업**: 중요한 시점에 메모리 상태 저장\n   ```python\n   backup = model.memory.data.detach().cpu().clone()\n   # ... 작업 수행 ...\n   model.memory.data = backup.to(model.device)\n   ```\n\n2. **메모리 리셋**: 새 세션 시작 시\n   ```python\n   model.initialized.fill_(0)\n   ```\n\n3. **배치 처리 주의**: 현재 batch_size=1만 안정적으로 지원\n\n### 성능 최적화\n\n- **Flash Attention 2**: 추론 속도 향상\n- **torch.float16**: 메모리 절약\n- **gradient_checkpointing**: 학습 시 메모리 절약",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n# 노트북 완료!\n\n## 전체 요약\n\n이 노트북에서 다룬 내용:\n\n### Part 1-4: Inference (추론)\n- MemoryLLM 아키텍처 및 Memory Pool 구조\n- `inject_memory()`, `update_memory_with_delta_memory()`, `drop_memory()`, `cat_memory_and_hiddens()`\n- Forward pass 및 텍스트 생성\n\n### Part 5-6: Training (학습)\n- PyTorch Lightning + DeepSpeed 기반 학습\n- `cat_and_drop_memory` 전략\n- LoRA 파라미터 효율화\n- 커리큘럼 학습 (`num_contexts_schedule`)\n\n### Part 7-8: Evaluation & Advanced (평가 및 고급)\n- Knowledge Retention 평가\n- M+ (MPlus) 확장 모델\n- Chat Model 사용법\n- Best Practices\n\n## Quick Reference\n\n```python\n# === Inference ===\nfrom modeling_memoryllm import MemoryLLM\n\nmodel = MemoryLLM.from_pretrained(\"YuWangX/memoryllm-8b\", torch_dtype=torch.float16)\ntokenizer = AutoTokenizer.from_pretrained(\"YuWangX/memoryllm-8b\")\n\n# 컨텍스트 주입\nmodel.inject_memory(tokenizer(context, return_tensors='pt').input_ids.cuda(), update_memory=True)\n\n# 생성\noutput = model.generate(tokenizer(question, return_tensors='pt').input_ids.cuda(), max_new_tokens=20)\nprint(tokenizer.decode(output[0]))\n\n# === Training ===\ncd train/\npython main.py -t --base MemoryLLM/configs/llama/llama_30x256.yaml\n\n# === Evaluation ===\npython test_qa_memory.py --model YuWangX/memoryllm-7b --nuc 5 --datasets naturalqa squad\n```\n\n## 참고 자료\n\n- **논문**: [MemoryLLM: Towards Self-Updatable Large Language Models](https://arxiv.org/abs/2402.04624) (ICML 2024)\n- **GitHub**: [YuWangX/MemoryLLM](https://github.com/YuWangX/MemoryLLM)\n- **HuggingFace**: [YuWangX/memoryllm-8b](https://huggingface.co/YuWangX/memoryllm-8b)",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}